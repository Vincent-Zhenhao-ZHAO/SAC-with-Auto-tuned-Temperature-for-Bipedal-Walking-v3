{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3gqHxg74d80",
        "outputId": "ad4a4a88-a664-4450-f4b4-db353346234d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 20.0 kB/114\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 37.3 kB/114\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [1 InRelease 66.3 kB/114\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Waiting for headers] [C\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\u001b[33m\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                    \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\u001b[0m\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\u001b[0m\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,496 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [995 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1,937 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:16 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,970 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,066 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,296 kB]\n",
            "Fetched 12.1 MB in 2s (6,674 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "23 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 23 not upgraded.\n",
            "Need to get 780 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.6 [780 kB]\n",
            "Fetched 780 kB in 0s (4,066 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 128126 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.20.13-1ubuntu1~20.04.6_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.6) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.6) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyglet==1.5.27\n",
            "  Downloading pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-1.5.27\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[box2d]==0.20.0\n",
            "  Downloading gym-0.20.0.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]==0.20.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]==0.20.0) (2.2.1)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyglet>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]==0.20.0) (1.5.27)\n",
            "Building wheels for collected packages: box2d-py, gym\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp38-cp38-linux_x86_64.whl size=2835035 sha256=6b691f54d97d796743a51cb042c8f30aa59e562c80fcf6c4aebf02750bc89a99\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/95/16/1dc99ff9a3f316ff245fdb5c9086cd13c35dad630809909075\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.20.0-py3-none-any.whl size=1650476 sha256=3add4cf5eac44fb1d607c9dd338e58cd38349c186de3631e2f48455dbb56d230\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/04/98/54d49e63bfcf3c83059832a76533f11f297a2030c397d82c6e\n",
            "Successfully built box2d-py gym\n",
            "Installing collected packages: box2d-py, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed box2d-py-2.3.5 gym-0.20.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay==3.0\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "# How to run:\n",
        "# For standard version: Run all cells in this notebook.\n",
        "# For hardcore version: \n",
        "# - Change the environment name to BipedalWalkerHardcore-v3 and change the hyperparameters.\n",
        "# - Run all cells in this notebook, \n",
        "\n",
        "# Code reference: \n",
        "# First version of SAC in BipedalWalker-v2: \n",
        "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic \n",
        "# SAC in BipedalWalker-v3:\n",
        "# https://github.com/CoderAT13/BipedalWalkerHardcore-SAC\n",
        "\n",
        "# Tuning reference:\n",
        "# rl-baseline-zoo: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/sac.yml\n",
        "\n",
        "# Paper reference:\n",
        "# First paper of SAC: https://arxiv.org/pdf/1801.01290.pdf\n",
        "# Second paper of SAC(with auto-tuned alpha): https://arxiv.org/pdf/1812.05905.pdf \n",
        "\n",
        "!apt update\n",
        "!apt install xvfb -y\n",
        "!pip install 'swig'\n",
        "!pip install 'pyglet==1.5.27'\n",
        "!pip install 'gym[box2d]==0.20.0'\n",
        "!pip install 'pyvirtualdisplay==3.0'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "\n",
        "plot_interval = 10 # update the plot every 10 episodes\n",
        "video_every = 100 # videos can take a very long time to render so only do it every 100 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xr5BgNPg4mb1"
      },
      "outputs": [],
      "source": [
        "# Code reference: \n",
        "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic \n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "epsilon = 1e-6\n",
        "\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
        "\n",
        "def hard_update(target, source):\n",
        "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jMadKJpE4n3x"
      },
      "outputs": [],
      "source": [
        "# Code reference: \n",
        "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic \n",
        "\n",
        "# Initialize Policy weights\n",
        "def weights_init_(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
        "        torch.nn.init.constant_(m.bias, 0)\n",
        "\n",
        "# Double Q-function trick:\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
        "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \n",
        "        xu = torch.cat([state, action], 1)\n",
        "        \n",
        "        x1 = F.relu(self.linear1(xu))\n",
        "        x1 = F.relu(self.linear2(x1))\n",
        "        x1 = self.linear3(x1)\n",
        "\n",
        "        x2 = F.relu(self.linear4(xu))\n",
        "        x2 = F.relu(self.linear5(x2))\n",
        "        x2 = self.linear6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "# Policy with reparameterization trick:\n",
        "class GaussianPolicy(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
        "        super(GaussianPolicy, self).__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
        "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
        "\n",
        "        self.apply(weights_init_)\n",
        "\n",
        "        # action rescaling\n",
        "        if action_space is None:\n",
        "            self.action_scale = torch.tensor(1.)\n",
        "            self.action_bias = torch.tensor(0.)\n",
        "        else:\n",
        "            self.action_scale = torch.FloatTensor(\n",
        "                (action_space.high - action_space.low) / 2.)\n",
        "            self.action_bias = torch.FloatTensor(\n",
        "                (action_space.high + action_space.low) / 2.)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        mean = self.mean_linear(x)\n",
        "        log_std = self.log_std_linear(x)\n",
        "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = log_std.exp()\n",
        "        normal = Normal(mean, std)\n",
        "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # Enforcing Action Bound\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_prob, mean\n",
        "\n",
        "    def to(self, device):\n",
        "        self.action_scale = self.action_scale.to(device)\n",
        "        self.action_bias = self.action_bias.to(device)\n",
        "        return super(GaussianPolicy, self).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ph1COoF74pmN"
      },
      "outputs": [],
      "source": [
        "# Code reference: \n",
        "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic \n",
        "\n",
        "# Replay buffer\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity, seed):\n",
        "        random.seed(seed)\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return state, action, reward, next_state, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-7G9uar_4rXO"
      },
      "outputs": [],
      "source": [
        "# Code reference: \n",
        "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic \n",
        "# https://github.com/CoderAT13/BipedalWalkerHardcore-SAC\n",
        "\n",
        "# SAC agent\n",
        "class SAC(object):\n",
        "    def __init__(self, num_inputs, action_space, \\\n",
        "                 device, hidden_size, lr, gamma, tau, alpha):\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.device = device \n",
        "\n",
        "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(device=self.device)\n",
        "        self.critic_optim = Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
        "        hard_update(self.critic_target, self.critic)\n",
        "        \n",
        "        # Target Entropy = −dim(A) as given in the paper\n",
        "        self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "        self.alpha_optim = Adam([self.log_alpha], lr=lr)\n",
        "        self.policy = GaussianPolicy(num_inputs, action_space.shape[0], \\\n",
        "                                         hidden_size, action_space).to(self.device)\n",
        "        self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
        "\n",
        "    # Add random noise when select actions\n",
        "    def select_action(self, state, eval=False):\n",
        "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
        "        if eval == False:\n",
        "            action, _, _ = self.policy.sample(state)\n",
        "        else:\n",
        "            _, _, action = self.policy.sample(state)\n",
        "        return action.detach().cpu().numpy()[0]\n",
        "\n",
        "    def update_parameters(self, memory, batch_size):\n",
        "        # Sample a batch from memory\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size=batch_size)\n",
        "\n",
        "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
        "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
        "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
        "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
        "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
        "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
        "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
        "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
        "\n",
        "        # Two Q-functions to mitigate positive bias in the policy improvement step\n",
        "        qf1, qf2 = self.critic(state_batch, action_batch) \n",
        "        qf1_loss = F.mse_loss(qf1, next_q_value) \n",
        "        qf2_loss = F.mse_loss(qf2, next_q_value) \n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "\n",
        "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
        "\n",
        "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
        "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "\n",
        "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() \n",
        "        \n",
        "        # Was been using by first version code, however due to bugs in pytorch\n",
        "        # Change it to qf_loss instead in second version.\n",
        "\n",
        "        # self.critic_optim.zero_grad()\n",
        "        # qf1_loss.backward()\n",
        "        # self.critic_optim.step()\n",
        "\n",
        "        # self.critic_optim.zero_grad()\n",
        "        # qf2_loss.backward()\n",
        "        # self.critic_optim.step()\n",
        "\n",
        "        self.policy_optim.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optim.step()\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        qf_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        # Auto-tune alpha\n",
        "        # In second version of the code has been commented for testing fixed alpha.\n",
        "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "\n",
        "        self.alpha_optim.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optim.step()\n",
        "\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        soft_update(self.critic_target, self.critic, self.tau)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zqUhA2aC4tJj"
      },
      "outputs": [],
      "source": [
        "# For standard version, if hardcore please comment line 2-15:\n",
        "gamma=0.99\n",
        "batch_size=256\n",
        "lr=5.3e-4 # learing rate\n",
        "hidden_size=400\n",
        "tau=0.02\n",
        "alpha=0.2 # suggested by author\n",
        "start_steps=10000\n",
        "update_start_steps=1e4\n",
        "reward_scale = 10 # experimented by myself, extremely important hyperparameter mentioned in paper\n",
        "test_ep = 10 # for adding noise\n",
        "max_timesteps = 2000\n",
        "capacity = 300000 # buffer size \n",
        "seed = 42\n",
        "iteration = 1100 # total episode\n",
        "\n",
        "# For hardcore version, please use uncomment line 20-32:\n",
        "# These hyperparameter was been used by second version of SAC code.\n",
        "\n",
        "# gamma=0.99\n",
        "# batch_size=256\n",
        "# lr=5e-4\n",
        "# hidden_size=400\n",
        "# tau=0.005\n",
        "# alpha=0.2\n",
        "# start_steps=10000\n",
        "# update_start_steps=1e4\n",
        "# reward_scale = 5\n",
        "# test_ep = 10\n",
        "# capacity = 1000000\n",
        "# seed = 42\n",
        "# iteration = 2200         # total episode, 2200 would reach over 300 a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yqZFpehC4w9D"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uZp86x9E4zK_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "# env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "# Set seeds\n",
        "env.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qX3TkXI_40vy"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLc48nm9pbgP"
      },
      "outputs": [],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(state_dim, action_dim))\n",
        "print('The device is: {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82TL_qL542O1"
      },
      "outputs": [],
      "source": [
        "# Code reference: \n",
        "# https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalker-Soft-Actor-Critic \n",
        "# https://github.com/CoderAT13/BipedalWalkerHardcore-SAC\n",
        "\n",
        "agent = SAC(state_dim, env.action_space, device, hidden_size, lr, gamma, tau, alpha)\n",
        "replay_buffer = ReplayMemory(capacity, seed)\n",
        "total_steps = 0 \n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "# i: each episode\n",
        "# ep_r: each episode reward\n",
        "# ep_s: steps in each episode\n",
        "for i in range(iteration):\n",
        "\n",
        "    ep_r = 0\n",
        "    ep_s = 0\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "\n",
        "        action = []\n",
        "        # Start to make observation\n",
        "        if total_steps < start_steps:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Stop to make observation\n",
        "            use_eval = False\n",
        "            if i % (test_ep*2) >= test_ep:\n",
        "                use_eval = True\n",
        "            action = agent.select_action(state, use_eval)\n",
        "\n",
        "        # get next action\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        ep_r += reward\n",
        "\n",
        "        # apply reward scaling\n",
        "        reward = reward * reward_scale\n",
        "\n",
        "        ep_s += 1\n",
        "        total_steps += 1\n",
        "        \n",
        "        mask = 1 if (ep_s == 1600) else float(not done)\n",
        "        \n",
        "        # add into replay_buffer\n",
        "        replay_buffer.push(state, action, reward, next_state, mask)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if ep_s > max_timesteps:\n",
        "            break\n",
        "    # update parameters\n",
        "    for upi in range(ep_s):\n",
        "        if len(replay_buffer) >= update_start_steps:\n",
        "            agent.update_parameters(replay_buffer, batch_size)\n",
        "    \n",
        "    # do NOT change this logging code - it is used for automated marking!\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(i, ep_r))\n",
        "    log_f.flush()\n",
        "  \n",
        "    reward_list.append(ep_r)\n",
        "\n",
        "    # plot graph\n",
        "    if i % plot_interval == 0:\n",
        "        plot_data.append([i, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
